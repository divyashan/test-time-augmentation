{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU:0\n",
      "Using GPU:1\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "import numpy as np\n",
    "sys.path.insert(0, '../')\n",
    "\n",
    "import torch\n",
    "from PIL import Image\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "from mnist_train import Net\n",
    "from utils.gpu_utils import restrict_GPU_pytorch\n",
    "from utils.imagenet_utils import accuracy\n",
    "from utils.dataloading_utils import MyIter, MyLoader\n",
    "from new_tta_models import ImageW, ImageS, ImageWS, Original, StandardTTA\n",
    "import torchvision.transforms.functional as F\n",
    "from cnn_finetune import make_model\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "restrict_GPU_pytorch('1')\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNetWrapper(\n",
       "  (_features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    (4): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (5): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (6): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (7): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pool): AdaptiveAvgPool2d(output_size=1)\n",
       "  (_classifier): Linear(in_features=512, out_features=102, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_flowers_model(model_name):\n",
    "    m_name = model_name\n",
    "    if model_name == 'MobileNetV2':\n",
    "        m_name = 'mobilenet_v2'\n",
    "    if model_name == 'inceptionv3':\n",
    "        m_name = 'inception_v3'\n",
    "    model = make_model(\n",
    "                    m_name,\n",
    "                    pretrained=True,\n",
    "                    num_classes=n_classes,\n",
    "                    input_size=(224, 224),\n",
    "                )\n",
    "    model.load_state_dict(torch.load('../saved_models/flowers102/' + m_name+ '.pth'))\n",
    "    return model\n",
    "n_classes = 102\n",
    "model = get_flowers_model('resnet18')\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Crop(object):\n",
    "    \"\"\"Crop randomly the image in a sample.\n",
    "\n",
    "    Args:\n",
    "        output_size (tuple or int): Desired output size. If int, square crop\n",
    "            is made.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, output_size, pos=4):\n",
    "        assert isinstance(output_size, (int, tuple))\n",
    "        self.pos = pos\n",
    "        if isinstance(output_size, int):\n",
    "            self.output_size = (output_size, output_size)\n",
    "        else:\n",
    "            assert len(output_size) == 2\n",
    "            self.output_size = output_size\n",
    "\n",
    "    def __call__(self, img):\n",
    "        return F.five_crop(img, self.output_size)[self.pos]\n",
    "\n",
    "class Scale(object):\n",
    "    \"\"\"Crop randomly the image in a sample.\n",
    "\n",
    "    Args:\n",
    "        output_size (tuple or int): Desired output size. If int, square crop\n",
    "            is made.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, output_size, pct=1):\n",
    "        assert isinstance(output_size, (int, tuple))\n",
    "        self.pct = pct\n",
    "        if isinstance(output_size, int):\n",
    "            self.output_size = (output_size, output_size)\n",
    "        else:\n",
    "            assert len(output_size) == 2\n",
    "            self.output_size = output_size\n",
    "\n",
    "    def __call__(self, img):\n",
    "        h, w = img.size\n",
    "        new_h, new_w = int(self.pct*h), int(self.pct*w)\n",
    "        img = F.center_crop(img, (new_h, new_w))\n",
    "        img = F.resize(img, self.output_size, Image.BILINEAR)\n",
    "        return img\n",
    "    \n",
    "class HFlip(object):\n",
    "    \"\"\"Crop randomly the image in a sample.\n",
    "\n",
    "    Args:\n",
    "        output_size (tuple or int): Desired output size. If int, square crop\n",
    "            is made.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, output_size):\n",
    "        assert isinstance(output_size, (int, tuple))\n",
    "        if isinstance(output_size, int):\n",
    "            self.output_size = (output_size, output_size)\n",
    "        else:\n",
    "            assert len(output_size) == 2\n",
    "            self.output_size = output_size\n",
    "\n",
    "    def __call__(self, img):\n",
    "        img = F.hflip(img)\n",
    "        img = F.center_crop(img, self.output_size)\n",
    "        return img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining dataloaders "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure out val/test index split for these \n",
    "def get_datapath(dataset_name):\n",
    "    if dataset_name == 'flowers102':\n",
    "        data_path = '../datasets/flowers102/test'\n",
    "    return data_path\n",
    "\n",
    "def get_augmentation_transform(augmentation):\n",
    "    # These could be compositions too\n",
    "    output_size = (224, 224)\n",
    "    aug_transform_map = {'hflip': HFlip(output_size),\n",
    "                         'crop_tl': Crop(output_size, 0),\n",
    "                         'crop_tr': Crop(output_size, 1),\n",
    "                         'crop_b1': Crop(output_size, 2),\n",
    "                         'crop_br': Crop(output_size, 3),\n",
    "                         'orig': Crop(output_size, 4),\n",
    "                         'scale_1.04': Scale(output_size, 1.04),\n",
    "                         'scale_1.10': Scale(output_size, 1.10)}\n",
    "    return aug_transform_map[augmentation]   \n",
    "\n",
    "def get_dataloader(dataset_name, augmentation, idxs, batch_size):\n",
    "    datapath = get_datapath(dataset_name)\n",
    "    if dataset_name == 'flowers102':\n",
    "        image_size = 256\n",
    "        crop_size = 224\n",
    "        normalize = transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        d_transforms = transforms.Compose([\n",
    "            transforms.Resize(image_size),\n",
    "            transforms.CenterCrop(image_size),\n",
    "            get_augmentation_transform(augmentation),\n",
    "            transforms.ToTensor(),\n",
    "            normalize\n",
    "        ])\n",
    "    dataset = datasets.ImageFolder(root=datapath, transform=d_transforms)\n",
    "    dataloader = torch.utils.data.DataLoader(torch.utils.data.Subset(dataset, idxs), \n",
    "                                             batch_size=batch_size,\n",
    "                                             num_workers=0, shuffle=False)\n",
    "        \n",
    "    # look up augmentation append it to transforms\n",
    "    return dataloader \n",
    "\n",
    "def get_dataloaders(dataset_name, augmentations, bs):\n",
    "    # Write val idxs\n",
    "    # Write test idxs\n",
    "    all_idxs = list(range(6149))\n",
    "    np.random.shuffle(all_idxs)\n",
    "    val_idxs = all_idxs[:int(len(all_idxs)/2)]\n",
    "    test_idxs = all_idxs[int(len(all_idxs)/2):]\n",
    "    train_dl_list = [get_dataloader(dataset_name, aug, val_idxs, bs) for aug in augmentations]\n",
    "    test_dl_list = [get_dataloader(dataset_name, aug, test_idxs, bs) for aug in augmentations]\n",
    "    return MyLoader(train_dl_list), MyLoader(test_dl_list)\n",
    "n_augs = 8\n",
    "n_classes = 102\n",
    "batch_size = 128\n",
    "data_loader, test_data_loader = get_dataloaders('flowers102', ['orig', 'hflip', 'crop_tl', 'crop_b1', \n",
    "                                                               'crop_tr', 'crop_br', \n",
    "                                                               'scale_1.04', 'scale_1.10'], 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# fig, axes = plt.subplots(2, 4, sharex=True, sharey=True)\n",
    "# for i in range(n_augs):\n",
    "#     img = x[i][1].cpu()\n",
    "#     ax = axes[int(i/4), i%4]\n",
    "#     ax.imshow(np.transpose(img, (1, 2, 0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Original model accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4751648febe443de80de22212814edc6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=97.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "88.14432989690722 97.1971649484536\n"
     ]
    }
   ],
   "source": [
    "orig_model = Original(model, 0)\n",
    "orig_model.cuda('cuda:0')\n",
    "orig_model.eval()\n",
    "test_acc1s = []\n",
    "test_acc5s = []\n",
    "for examples, target in tqdm(test_data_loader):\n",
    "    examples = examples.cuda('cuda:0', non_blocking=True)\n",
    "    target = target.cuda('cuda:0', non_blocking=True)    \n",
    "    output = orig_model(examples)\n",
    "    acc1, acc5 = accuracy(output, target, topk=(1, 5))\n",
    "    test_acc1s.append(acc1.item())\n",
    "    test_acc5s.append(acc5.item())\n",
    "print(np.mean(test_acc1s), np.mean(test_acc5s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating Standard TTA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd29d31687a24e4ebeec9fadef73ba4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=97.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "88.2409793814433 97.06829896907216\n"
     ]
    }
   ],
   "source": [
    "stta_model = StandardTTA(model)\n",
    "stta_model.cuda('cuda:0')\n",
    "test_acc1s = []\n",
    "test_acc5s = []\n",
    "for examples, target in tqdm(test_data_loader):\n",
    "    examples = examples.cuda('cuda:0', non_blocking=True)\n",
    "    target = target.cuda('cuda:0', non_blocking=True)    \n",
    "    output = stta_model(examples)\n",
    "    acc1, acc5 = accuracy(output, target, topk=(1, 5))\n",
    "    test_acc1s.append(acc1.item())\n",
    "    test_acc5s.append(acc5.item())\n",
    "print(np.mean(test_acc1s), np.mean(test_acc5s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.aug_utils import invert_aug_list\n",
    "\n",
    "aug_list = np.load('../' + 'flowers102' + '/' + 'five_crop_hflip_scale' + '/aug_list.npy')\n",
    "aug_order = np.load('../' +'flowers102' + '/' + 'five_crop_hflip_scale' + '/aug_order.npy')\n",
    "aug_names = invert_aug_list(aug_list, aug_order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([12])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(np.sum(aug_list,axis=1) == 0)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating... all models together "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_agg_model(agg_model, dataloader):\n",
    "    agg_model.eval()\n",
    "    agg_model.cuda('cuda:0')\n",
    "    model.cuda('cuda:0')\n",
    "    test_acc1s = []\n",
    "    test_acc5s = []\n",
    "    outputs = []\n",
    "    targets = []\n",
    "    for examples, target in dataloader:\n",
    "        examples = examples.cuda('cuda:0', non_blocking=True)\n",
    "        target = target.cuda('cuda:0', non_blocking=True)    \n",
    "        output = agg_model(examples)\n",
    "        acc1, acc5 = accuracy(output, target, topk=(1, 5))\n",
    "        test_acc1s.append(acc1.item())\n",
    "        test_acc5s.append(acc5.item())\n",
    "        outputs.append(output.cpu())\n",
    "        targets.append(target.cpu())\n",
    "    return np.mean(test_acc1s), np.mean(test_acc5s)\n",
    "\n",
    "\n",
    "def train_agg_model(model, dataloader, n_augs, n_classes, agg_model_name, n_features, orig_idx):\n",
    "    if agg_model_name == 'image_s':\n",
    "        agg_model = ImageS(model, n_classes, n_features, orig_idx)\n",
    "    elif agg_model_name == 'image_w':\n",
    "        agg_model = ImageW(model, n_augs, n_classes, n_features, orig_idx)\n",
    "\n",
    "    agg_model.cuda('cuda:0')\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    criterion.cuda('cuda:0')\n",
    "    optimizer = torch.optim.SGD(agg_model.parameters(), lr=.01, momentum=.9, weight_decay=1e-4)\n",
    "\n",
    "    losses = []\n",
    "    acc1s = []\n",
    "    acc5s = []\n",
    "    epochs = 100\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        epoch_loss = []\n",
    "        for examples, target in tqdm(dataloader):\n",
    "            examples = examples.cuda('cuda:0', non_blocking=True)\n",
    "            target = target.cuda('cuda:0', non_blocking=True)\n",
    "            output = agg_model(examples)\n",
    "            loss = criterion(output, target)\n",
    "            acc1, acc5 = accuracy(output, target, topk=(1, 5))\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss.append(loss.item())\n",
    "            acc1s.append(acc1.item())\n",
    "            acc5s.append(acc5.item())\n",
    "        losses.append(np.mean(epoch_loss))\n",
    "    return agg_model, losses, acc1s, acc5s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "179b1bbf7cb44983b25e0501c05eb85f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94c953a654274a65ba73eed697946078",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=97.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c7818fafc2b4613b4d68624a8152031",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=97.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bcda1535d3ca41f2a20c1404a054aa11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=97.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6baf680e52084c95a63129921589b039",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=97.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50ede3e48e5f472cac9e65314c03e37b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=97.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c318a99b662c4eee8c90255d68d2a5c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=97.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39f1ced81c384777a968c243b9ec2b9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=97.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Original model\n",
    "orig_idx = 0\n",
    "results = []\n",
    "# orig_model = Original(model, 0)\n",
    "# orig_acc1, orig_acc5 = eval_agg_model(orig_model, test_data_loader)\n",
    "# results.append({'agg_model': 'orig',  'acc1': orig_acc1, 'acc5': orig_acc5})\n",
    "# # stta model\n",
    "# stta_model = StandardTTA(model)\n",
    "# stta_acc1, stta_acc5 = eval_agg_model(stta_model, test_data_loader)\n",
    "# results.append({'agg_model': 'stta', 'acc1': stta_acc1, 'acc5': stta_acc5})\n",
    "\n",
    "n_features = 25088\n",
    "# image to s \n",
    "s_model, losses, acc1s, acc5s = train_agg_model(model, data_loader, n_augs, n_classes, \n",
    "                                                'image_s', n_features, orig_idx)\n",
    "acc1, acc5, s_outputs, s_targets = eval_agg_model(s_model, test_data_loader)\n",
    "results.append({'agg_model': 'image_s', 'acc1': acc1, 'acc5': acc5})\n",
    "\n",
    "# image to w \n",
    "w_model, losses, acc1s, acc5s = train_agg_model(model, data_loader, n_augs, n_classes, \n",
    "                                                'image_w', n_features, orig_idx)\n",
    "acc1, acc5, w_outputs, w_targets = eval_agg_model(w_model, test_data_loader)\n",
    "results.append({'agg_model': 'image_w',  'acc1': acc1, 'acc5': acc5})\n",
    "\n",
    "# image to s and w \n",
    "# ws_model, losses, acc1s, acc5s = train_agg_model(model, data_loader, n_augs, 'image_ws', n_features)\n",
    "# acc1, acc5, ws_outputs, ws_targets = eval_agg_model(ws_model, test_data_loader)\n",
    "# results.append({'agg_model': 'image_ws', 'acc1': acc1, 'acc5': acc5})\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.to_csv('flowers102_preliminary_results')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
